\week{18}{29 Jun.\ 2024}{Tighter Bounds on Sub-Gaussian Matrices}

\newcommand\BIN\mathbin
\newcommand\RM\mathrm
\newcommand\IT\mathit
\newcommand\FR\mathfrak
\newcommand\BBM\mathbbm
\newcommand\Unit[1]{\,\RM{#1}}
\newcommand\PU[1]{/\RM{#1}}
\newcommand\DC{\Unit{^\circ C}}
\newcommand\RMDC{\RM{^\circ C}}
\newcommand\DD[1]{\mathop{\mathrm{d}#1}}
\newcommand\EQ[1]{\begin{equation}#1\end{equation}}
\newcommand\EQN[2]{\EQ{#2\label{#1}}}
\newcommand\BM[1]{\boldsymbol{#1}}
\newcommand\BB[1]{\mathbb{#1}}
\newcommand\CAL[1]{\mathcal{#1}}
\newcommand\Tp{^\mathsf{T}}
\newcommand\Hp{^\mathsf{H}}
\newcommand\To[1]{\overset{\mathrm{#1}}{\to}}
\newcommand\UB[1]{^{(#1)}}
\newcommand\OP[1]{\operatorname{#1}}
\newcommand\MAT[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand\AL[1]{\begin{align}#1\end{align}}
\newcommand\AM[1]{\begin{align*}#1\end{align*}}
\newcommand\ALN[2]{\AL{#2\label{#1}}}
\newcommand\GA[1]{\begin{gather}#1\end{gather}}
\newcommand\GB[1]{\begin{gather*}#1\end{gather*}}
\newcommand\HAT[1]{\widehat{#1}}
\newcommand\BAR[1]{\overline{#1}}
\newcommand\TLD[1]{\widetilde{#1}}
%\newcommand\Prb{\mathbb{P}}
%\newcommand\Exp{\mathbb{E}}
\newcommand\Tr{\operatorname{tr}}
\newcommand\Var{\operatorname{Var}}
\newcommand\Cov{\operatorname{Cov}}
\newcommand\IID{\overset{\mathrm{i.i.d.}}{\sim}}
\newcommand\VS{\longleftrightarrow}
\newcommand\diag{\operatorname{diag}}
\DeclareMathOperator*{\Prb}{\mathbb{P}}
\DeclareMathOperator*{\Exp}{\mathbb{E}}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\softmax}{softmax}
\DeclareMathOperator*{\hardmax}{hardmax}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}

\section{Matrix Bernsteinâ€™s inequality}
\begin{problem*}[Exercise 5.4.3]\label{ex5.4.3}

\end{problem*}
\begin{answer}
Let $\BM X=:\BM U\BM\varLambda\BM U\Tp$ be the symmetric eigendecomposition of $\BM X$. 

\begin{enumerate}[(a)]
\item
Since $\BM X^k=\BM U\BM\varLambda\BM U\Tp\cdots\BM U\BM\varLambda\BM U\Tp=\BM U\BM\varLambda\BM I\cdots\BM I\BM\varLambda\BM U\Tp=\BM U\BM\varLambda^k\BM U\Tp$ for all $k\ge0$, then
\AM{
f(\BM X)=\BM Uf(\BM\varLambda)\BM U\Tp=\BM U\bigg(\sum_{k=0}^pa_k\BM\varLambda^k\bigg)\BM U\Tp=\sum_{k=0}^pa_k\BM U\BM\varLambda^k\BM U\Tp=\sum_{k=0}^pa_k\BM X^k
.}
\item
Since $\BM X-x_0\BM I=\BM U(\BM\varLambda-x_0\BM I)\BM U\Tp$, then by (a),
\AM{
f(\BM X)=\BM U\bigg(\sum_{k=1}^\infty a_k(\BM\varLambda-x_0\BM I)^k\bigg)\BM U\Tp=\sum_{k=1}^\infty a_k\BM U(\BM\varLambda-x_0\BM I)^k\BM U\Tp=\sum_{k=0}^\infty a_k(\BM X-x_0\BM I)^k
.}
\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.5]\label{ex5.4.5}

\end{problem*}
\begin{answer}
Let $\BM X=:\BM U\BM\varLambda\BM U\Tp$ and $\BM Y=:\BM V\BM M\BM V\Tp$ denote the symmetric eigendecompositions of $\BM X$ and $\BM Y$, respectively. Additionally, let $\BM\lambda:=\diag(\BM\varLambda)$, $\BM\mu:=\diag(\BM M)$.

\begin{enumerate}[(a)]
\item
By the Courant--Fisher min-max theorem w.r.t.\ $\lambda_1$ and $\lambda_n$,
\AM{
\|\BM X\|\le t&\iff -t\BM1\le\BM\lambda\le t\BM1\iff t\BM1\pm\BM\lambda\ge\BM0\iff t\BM I\pm\BM X\succeq\BM0\iff -t\BM I\preceq\BM X\preceq t\BM I
.}

\item
Since $|\BM\lambda|\le K\BM1$, then $g(\BM\lambda)-f(\BM\lambda)\ge\BM0$. This implies that $g(\BM X)-f(\BM X)=\BM U(g(\BM\varLambda)-f(\BM\varLambda))\BM U\Tp$ has non-negative eigenvalues. Therefore, $g(\BM X)\succeq f(\BM X)$.

\item
Since $\BM X$ and $\BM Y$ are symmetric and commute, then $\BM Y$ admits an eigendecomposition with $\BM V=\BM U$. This implies $\BM\lambda\le\BM\mu$. It follows that $f(\BM\mu)-f(\BM\lambda)\ge0$, so
$f(\BM Y)-f(\BM X)=\BM U(f(\BM M)-f(\BM\varLambda))\BM U\Tp$ has non-negative eigenvalues. Therefore, $f(\BM X)\preceq f(\BM Y)$.

\item
\AM{
\textstyle
\lambda\bigg(\!\MAT{4&2\\2&4}-\MAT{3&0\\0&0}\!\bigg)\!=\{5,0\},\quad\lambda\bigg(\!\MAT{4&2\\2&4}^{\!3}-\MAT{3&0\\0&0}^{\!3}\!\bigg)\!=\big\{\!\frac{\sqrt{43993}+197}2,-\frac{\sqrt{43993}-197}2\!\big\}.}

\item
Since $\BM X-\BM Y\preceq\BM0$, then by the Courant--Fisher min-max theorem, for any $i=1,\dots,n$,
\AM{
\lambda_i-\mu_i&=\max_{\OP{dim}\CAL E=i}\min_{\BM v\in\BB S(\CAL E)}\BM v\Tp\BM X\BM v-\max_{\OP{dim}\CAL E=i}\min_{\BM v\in\BB S(\CAL E)}\BM v\Tp\BM Y\BM v\\
&\le\max_{\OP{dim}\CAL E=i}\Big(\min_{\BM v\in\BB S(\CAL E)}\BM v\Tp\BM X\BM v-\min_{\BM v\in\BB S(\CAL E)}\BM v\Tp\BM Y\BM v\Big)\\
%&=\max_{\OP{dim}\CAL E=i}\Big(\min_{\BM v\in\BB S(\CAL E)}\BM v\Tp\BM X\BM v+\max_{\BM v\in\BB S(\CAL E)}\big({-\BM v\Tp\BM Y\BM v}\big)\Big)\\
&\le\max_{\OP{dim}\CAL E=i}\max_{\BM v\in\BB S(\CAL E)}\big(\BM v\Tp\BM X\BM v-\BM v\Tp\BM Y\BM v\big)\\
&=\max_{\OP{dim}\CAL E=i}\max_{\BM v\in\BB S(\CAL E)}\BM v\Tp(\BM X-\BM Y)\BM v\le0
.}
Since $f$ is increasing, then $f(\lambda_i)\le f(\mu_i)$ for all $i$. It follows that
\AM{\Tr f(\BM X)=\sum_{i=1}^nf(\lambda_i)\le\sum_{i=1}^nf(\mu_i)=\Tr f(\BM Y).}

\item
Since $\BM X\preceq\BM Y$, then $\BM I=\BM X^{-1/2}\BM X\BM X^{-1/2}\preceq\BM X^{-1/2}\BM Y\BM X^{-1/2}$. This implies $\lambda(\BM X^{-1/2}\BM Y\BM X^{-1/2})\ge1$. Thus, $\lambda(\BM X^{1/2}\BM Y^{-1}\BM X^{1/2})=\lambda^{-1}(\BM X^{-1/2}\BM Y\BM X^{-1/2})\le1$, so $\BM X^{1/2}\BM Y^{-1}\BM X^{1/2}\preceq\BM I$. It follows that $\BM Y^{-1}=\BM X^{-1/2}(\BM X^{1/2}\BM Y^{-1}\BM X^{1/2})\BM X^{-1/2}\preceq\BM X^{-1/2}\BM I\BM X^{-1/2}=\BM X^{-1}$. 

\item
By (f), $(\BM X+t\BM I)^{-1}\succeq(\BM Y+t\BM I)^{-1}$ for $t\ge0$. Since $\log z=\log\frac{1+t}{z+t}\big|_{t=0}^\infty=\int_0^\infty\big(\frac1{1+t}-\frac1{z+t}\big)\DD t$, then
\AM{\log\BM X=\int_0^\infty((1+t)^{-1}\BM I-(\BM X+t\BM I)^{-1})\DD t\preceq\int_0^\infty((1+t)^{-1}\BM I-(\BM Y+t\BM I)^{-1})\DD t=\log\BM Y.}
\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.6]\label{ex5.4.6}

\end{problem*}
\begin{answer}
\begin{enumerate}[(a)]
\item
Since $\BM X$ and $\BM Y$ commute, then by the binomial theorem and the substitution $i:=k-j$,
\AM{\RM e^{\BM X+\BM Y}=\sum_{k=0}^\infty\frac{(\BM X+\BM Y)^k}{k!}=\sum_{k=0}^\infty\frac{1}{k!}\sum_{j=0}^k\frac{k!}{(k-j)!j!}\BM X^{k-j}\BM Y^{j}=\sum_{i=0}^\infty\frac{\BM X^i}{i!}\sum_{j=0}^\infty\frac{\BM Y^j}{j!}=\RM e^{\BM X}\RM e^{\BM Y}.}

\item
For $\BM X:=\MAT{1&0\\0&-1}$ and $\BM Y:=\MAT{0&1\\1&0}$,
\AM{%\textstyle
\RM e^{\BM X+\BM Y}\!=\!\MAT{\cosh\sqrt2+\frac1{\sqrt2}{\sinh\sqrt2}&\!\!\!\!\!\!\!\!\frac1{\sqrt2}{\sinh\sqrt2}\\\frac1{\sqrt2}{\sinh\sqrt2}&\!\!\!\!\!\!\!\!\cosh\sqrt2-\frac1{\sqrt2}{\sinh\sqrt2}}\!,\quad
\RM e^{\BM X}\RM e^{\BM Y}\!=\frac12\!\MAT{\RM e^2+1&\RM e^2-1\\1-\RM e^{-2}&1+\RM e^{-2}}\!
.}
\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.11]\label{ex5.4.11}

\end{problem*}
\begin{answer}
Let $\sigma^2:=\|{\sum_{i=1}^N\Exp\BM X_i^2}\|$. By the matrix Berstein's inequality, for every $u>0$, with the substitution $t:=c^{-\frac12}\sigma\sqrt{u+\ln n}+c^{-1}K(u+\ln n)$,
\AM{\Prb\!\bigg\{{\bigg\|{\sum_{i=1}^N\BM X_i}\bigg\|\ge t}\bigg\}\le2n\RM e^{-c\min\left\{\!\frac{t^2}{\sigma^2},\frac{t}{K}\!\right\}}\le2n\RM e^{-(u+\ln n)}=2\RM e^{-u}.}
Then by Lemma 1.2.1,
\AM{\Exp\bigg\|{\sum_{i=1}^N\BM X_i}\bigg\|&=\bigg(\int_0^{c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)}+\int_{c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)}^\infty\bigg)\Prb\!\bigg\{{\bigg\|{\sum_{i=1}^N\BM X_i}\bigg\|\ge t}\bigg\}\DD t\\
&\le\int_0^{c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)}1\DD t+\int_{c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)}^\infty2\RM e^{-u}\DD t\\
&=c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)+\int_1^\infty2\RM e^{-u}\Big(\frac{2^{-1}c^{-\frac12}\sigma}{\sqrt{u+\ln n}}+c^{-1}K\Big)\DD u\\
&\le c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)+\int_1^\infty2\RM e^{-u}\Big(\frac{2^{-1}c^{-\frac12}\sigma}{\sqrt{1+\ln n}}+c^{-1}K\Big)\DD u\\
&=c^{-\frac12}\sigma\sqrt{1+\ln n}+c^{-1}K(1+\ln n)+2\RM e^{-1}\Big(\frac{2^{-1}c^{-\frac12}\sigma}{\sqrt{1+\ln n}}+c^{-1}K\Big)\\
&\lesssim\sigma\sqrt{1+\ln n}+K(1+\ln n)
.}
\end{answer}

\begin{problem*}[Exercise 5.4.12]\label{ex5.4.12}

\end{problem*}
\begin{answer}
Let $\sigma^2:=\|{\sum_{i=1}^N\BM A_i^2}\|$ and $\lambda:=\frac{t}{\sigma^2}\ge0$. By Exercise 2.2.3,
\AM{&\Tr\RM e^{\sum_{i=1}^N\!\log\Exp\RM e^{\lambda\varepsilon_i\BM A_i}}
=\Tr\RM e^{\sum_{i=1}^N\!\log\cosh(\lambda\BM A_i)}
\le\Tr\RM e^{\sum_{i=1}^N\!\frac{\lambda^2}2\BM A_i^2}\le n\RM e^{\frac{\lambda^2}2\lambda_{\max}(\sum_{i=1}^N\BM A_i^2)}=n\RM e^{\frac{\lambda^2\sigma^2}2}
.}
Then by the Chernoff bound and Lieb's inequality,
\AM{{}&\Prb\!\bigg\{\lambda_{\max}\!\bigg({\sum_{i=1}^N\varepsilon_i\BM A_i}\!\bigg)\!\ge t\bigg\}\!\le\RM e^{-\lambda t}\Exp\RM e^{\lambda\cdot\lambda_{\max}(\sum_{i=1}^N\varepsilon_i\BM A_i)}\!
=\RM e^{-\lambda t}\Tr\RM e^{\sum_{i=1}^N\!\log\Exp\RM e^{\lambda\varepsilon_i\BM A_i}}\!\le\RM e^{-\lambda t}n\RM e^{\frac{\lambda^2\sigma^2}2}\!=n\RM e^{-\frac{t^2}{2\sigma^2}}
.}
Similarly, $\Prb\{\lambda_{\min}(\sum_{i=1}^N\varepsilon_i\BM A_i)\le -t\}\le n\RM e^{-\frac{t^2}{2\sigma^2}}$.
\end{answer}

\begin{problem*}[Exercise 5.4.13]\label{ex5.4.13}

\end{problem*}
\begin{answer}
Since (a) follows from (b) with $p=1$, I will present only (b) here.

Since the inequality trivially holds for $n=1$ with $C=1$, let's assume $n\ge2$ from now on.

Note that if $1\le p\le2$, then by Stirling's approximation $\Gamma(z)\le\sqrt{\frac{2\uppi}z}\big(\frac z{\RM e}\big)^{\!z}\RM e^{\frac1{12z}}$,
\AM{\bigg(\!\int_0^\infty\!\!\RM e^{-s}(\ln(2n)+s)^{\frac p2-1}\DD s\!\bigg)^{\!\frac1p}\!\!
&\le\bigg(\!\int_0^\infty\!\!\RM e^{-s}(0+s)^{\frac p2-1}\DD s\!\bigg)^{\!\frac1p}\!\!
=\Gamma\Big(\frac p2\Big)^{\!\frac1p}\le\frac{\uppi^{\frac1{2p}}p^{\frac{p-1}{2p}}}{2^{\frac12-\frac1p}\RM e^{\frac12-\frac1{6p^2}}};}
and that if $p>2$, then by Minkowski's inequality (and the same Stirling's approximation),
\AM{
&\bigg(\!\int_0^\infty\!\!\RM e^{-s}(\ln(2n)+s)^{\frac p2-1}\DD s\!\bigg)^{\!\frac1p}\!\!
=\bigg(\!\int_0^\infty\!\!\big(\RM e^{-\frac sp}(\ln(2n)+s)^{\frac12-\frac1p}\big)^{p}\DD s\!\bigg)^{\!\frac1p}\!\!\\
\le{}&\bigg(\!\int_0^\infty\!\!(\RM e^{-\frac sp}((\ln(2n))^{\frac12-\frac1p}+s^{\frac12-\frac1p})^{p}\DD s\!\bigg)^{\!\frac1p}\!\!
\le\bigg(\!\int_0^\infty\!\!(\RM e^{-\frac sp}(\ln(2n))^{\frac12-\frac1p})^p\DD s\!\bigg)^{\!\frac1p}\!\!+\bigg(\!\int_0^\infty\!\!(\RM e^{-\frac sp}s^{\frac12-\frac1p})^{p}\DD s\!\bigg)^{\!\frac1p}\!\!\\
={}&(\ln(2n))^{\frac12-\frac1p}\!\bigg(\!\int_0^\infty\!\!\!\!\RM e^{-s}\DD s\!\!\bigg)^{\!\frac1p}\!\!\!+\!\bigg(\!\int_0^\infty\!\!\!\!\RM e^{-s}s^{\frac p2-1}\DD s\!\!\bigg)^{\!\frac1p}\!\!\!
=(\ln(2n))^{\frac12-\frac1p}\!+\Gamma\Big(\frac p2\Big)^{\!\frac1p}\!\!
\le(\ln(2n))^{\frac12-\frac1p}\!+\frac{\uppi^{\frac1{2p}}p^{\frac{p-1}{2p}}}{2^{\frac12-\frac1p}\RM e^{\frac12-\frac1{6p^2}}}
.}
Let $\sigma^2:=\|\!\sum_{i=1}^N\BM A_i^2\|$. By Exercise~5.4.12, for any $t\ge0$,
\AM{\Prb\!\bigg\{\bigg\|\sum_{i=1}^N\varepsilon_i\BM A_i\bigg\|^p\!\ge t\bigg\}=\Prb\!\bigg\{\bigg\|\sum_{i=1}^N\varepsilon_i\BM A_i\bigg\|\ge t^{\frac1p}\!\bigg\}\le2n\RM e^{-\frac{t^{2/p}}{2\sigma^2}}.}
Then with the substitution $t=:\big(\sigma\sqrt{2(\ln(2n)+s)}\big)^p$, by Lemma~1.2.1 and Minkowski's inequality,
\AM{
\bigg(\!\Exp\bigg\|\sum_{i=1}^N\varepsilon_i\BM A_i\bigg\|^{p}\bigg)^{\!\frac1p}&=\bigg(\!\bigg(\!\int_0^{(\sigma\sqrt{2\ln(2n)})^p}\!\!\!\!+\int_{(\sigma\sqrt{2\ln(2n)})^p}^\infty\!\bigg)\Prb\!\bigg\{\bigg\|\sum_{i=1}^N\varepsilon_i\BM A_i\bigg\|^p\!\ge t\bigg\}\DD t\!\bigg)^{\!\frac1p}\\
&\le\bigg(\!\int_0^{(\sigma\sqrt{2\ln(2n)})^p}\!\!\!\!1\DD t+\int_{(\sigma\sqrt{2\ln(2n)})^p}^\infty\!\!2n\RM e^{-\frac{t^{2/p}}{2\sigma^2}}\DD t\!\bigg)^{\!\frac1p}\\
&=\bigg(\!\big(\sigma\sqrt{2\ln(2n)}\big)^p+\int_0^\infty\!\!\RM e^{-s}\frac{(\sqrt{2}\sigma)^pp}2(\ln(2n)+s)^{\frac p2-1}\DD s\!\bigg)^{\!\frac1p}\\
&=\sqrt2\sigma\bigg(\!\sqrt{\ln(2n)}^{p}+\frac{p}2\!\int_0^\infty\!\!\RM e^{-s}(\ln(2n)+s)^{\frac p2-1}\DD s\!\bigg)^{\!\frac1p}\\
&\le\sqrt2\sigma\bigg(\!\sqrt{\ln(2n)}+\Big(\frac{p}2\Big)^{\!\frac1p}\!\bigg(\!\int_0^\infty\!\!\RM e^{-s}(\ln(2n)+s)^{\frac p2-1}\DD s\!\bigg)^{\!\frac1p}\!\bigg)\\
&\le\sqrt2\sigma\bigg(\!\sqrt{\ln(2n)}+\Big(\frac{p}2\Big)^{\!\frac1p}\!\Big((\ln(2n))^{\frac12-\frac1p}\BBM1_{[p>2]}\!+\frac{\uppi^{\frac1{2p}}p^{\frac{p-1}{2p}}}{2^{\frac12-\frac1p}\RM e^{\frac12-\frac1{6p^2}}}\Big)\!\bigg)\\
&=\sqrt2\sigma\bigg(\!\Big(1+\Big(\frac{p}2\Big)^{\!\frac1p}(\ln(2n))^{-\frac1p}\BBM1_{[p>2]}\Big)\!\sqrt{\ln(2n)}+\frac{\uppi^{\frac1{2p}}p^{\frac{p+1}{2p}}}{\sqrt2\RM e^{\frac12-\frac1{6p^2}}}\Big)\!\bigg)\\
&\le\sqrt2\sigma\bigg(\big(1+\RM e^{\!\frac1{\RM e\ln16}}\big)\sqrt{\ln(2n)}+\frac{\sqrt\uppi}{\sqrt2\RM e^{\frac13}}\sqrt p\!\bigg)\\
&\asymp\sigma\sqrt{p+\ln n}
.}
\end{answer}

\begin{problem*}[Exercise 5.4.14]\label{ex5.4.14}
	Let \(X\) be an \(n\times n\) random matrix that takes values \(e_ke_k^{\top}\), \(k=1,\dots,n\), with probability \(1/n\) each. (Here \((e_k)\) denotes the standard basis in \(\mathbb{R}^n\).) Let \(X_1,\dots,X_N\) be independent copies of \(X\). Consider the sum \(S=\sum_{i=1}^N X_i\), which is a diagonal matrix.
	\begin{enumerate}[(a)]
		\item\label{ex5.4.14:a} Show that the entry \(S_{ii}\) has the same distribution as the number of balls in \(i\)-th bin when \(N\) balls are thrown into \(n\) bins independently.
		\item\label{ex5.4.14:b} Relating this to the classical coupon collector's problem, show that if \(N \asymp n\), then
		      \[
			      \mathbb{E}\Vert S \Vert\asymp \frac{\log n}{\log\log n}.
		      \]
		      Deduce that the bound in \hyperref[ex5.4.11]{Exercise 5.4.11} would fail if the logarithmic factors were removed from it.
	\end{enumerate}
\end{problem*}
\begin{answer}
	\begin{enumerate}[(a)]
		\item We see that \(X = e_k e_k ^{\top} \) with \(k\) being chosen uniformly randomly among \([n]\), where \(e_k e_k ^{\top} \) is a matrix with all \(0\)'s except the \(k^{\text{th} }\) diagonal element being \(1\). Hence, by interpreting each \(X_i\) as ``throwing a ball into \(n\) bins,'' \(S_{k k}\) records the number of balls in the \(k^{\text{th} }\) bin when \(N\) balls are thrown into \(n\) bins independently.
		\item We first observe that since \(S\) is diagonal, \(\lVert S \rVert = \lambda _1(S) = \max _k S_{k k}\) as all the diagonal elements are eigenvalues of \(S\). We first answer the question of how this related to the coupon collector's problem. Firstly, let's introduce the problem formally:

		      \begin{problem}[Coupon collector's problem]
		      Say we have \(n\) different types of coupons to collect, and we buy \(N\) boxes, where each box contains a (uniformly) random type of coupon. The classical \emph{coupon collector problem} asks for the expected number of boxes (i.e., \(N\)) we need in order to collect all coupons.
		      \end{problem}

		      With the interpretation of \hyperref[ex5.4.14:a]{(a)}, we can view \(S_{k k}\) as the number of coupons we have collected for the \(k^{\text{th}}\) type of the coupon, where \(N\) is the number of boxes we have bought. Hence, the coupon collector's problem asks for the expected \(N\) we need for \(\lambda _n (S) = \min _{k} S_{k k} > 0\).

		      Compared to our original problem, we see that it is \emph{different} from the classical coupon collector's problem: we're asked for the expected number of \emph{the most frequent} coupons (i.e., \(\max_{k} S_{k k}\)) we will see when buying only \(N \asymp n\) boxes.

		      To proceed, we need to recall the following theorems.

		      \begin{theorem}[Poisson limit thoerem]\label{thm:Poisson-limit}
			      Let \(X_{N, i}\), \(1 \leq i \leq N\), be independent random variables \(X_{N, i} \sim \operatorname{Ber}(p_{N, i}) \), and let \(S_N \coloneqq \sum_{i=1}^{N} X_{N, i}\). Assume that as \(N \to \infty \), \(\max _{i \leq N} p_{N, i} \to 0\) and \(\mathbb{E}_{}[S_N] \to \lambda < \infty \). Then, as \(N \to \infty \), \(S_N \overset{D}{\to} \operatorname{Pois}(\lambda ) \).\footnote{Recall that \(Z \sim \operatorname{Pois}(\lambda ) \) means \(\mathbb{P} (Z = k) = e^{-\lambda } \lambda ^k / k!\) for \(k = 0, 1, \dots \).}
		      \end{theorem}

		      Also, we need the following.

		      \begin{theorem}[Skorokhod's representation theorem]\label{thm:Skorokhod-representation}
			      If \(X_N \overset{D}{\to} X\), then there exists \((\Omega , \mathscr{F} , \mathbb{P} )\) on which we can define random variables \((Y_n)\) and \(Y\) such that \(Y_N \overset{D}{=} X_N\) for all \(n\) and \(Y \overset{D}{=} X\), and \(\mathbb{P} (Y_N \to Y) = 1\).
		      \end{theorem}

		      Now, consider the following random \(n \times n\) matrix (slightly different from \(S\))
		      \[
			      T \coloneqq \sum_{i=1}^{N} \sum_{k=1}^{n} b_{ik}^{(N)} e_k e_k ^{\top} ,
		      \]
		      where \(b_{ik}^{(N)} \overset{\text{i.i.d.} }{\sim } \operatorname{Ber}(1 / N) \). Using the above theorems, we can show the following.

		      \begin{claim}
			      As \(N \to \infty \), with \(Z_k \sim \operatorname{Pois}(1) \), we have
			      \[
				      \mathbb{E}_{}[\lambda _1(T)]
				      = \mathbb{E}_{}\left[ \max _{1 \leq k \leq n} T_{k k} \right]
				      \to \mathbb{E}_{}\left[ \max _{1 \leq k \leq n} Z_k \right]
				      = \Theta \left( \frac{\log n}{\log \log n} \right).
			      \]
		      \end{claim}
		      \begin{explanation}
			      For every \(k \in [n]\), we apply the \hyperref[thm:Poisson-limit]{Poisson limit theorem} since as \(N \to \infty \), \(p_{N, ik} = 1 / N \to 0\) and \(\mathbb{E}_{}[S^k_N] = \mathbb{E}_{}[\sum_{i=1}^{N} b^{(N)}_{ik}] = 1 \eqqcolon \lambda \) as \(N \to \infty \). So as \(N \to \infty \), \(S^k_N \overset{D}{\to} \operatorname{Pois}(1) \).

			      Here, with a similar interpretation as in \hyperref[ex5.4.14:a]{(a)}, we can interpret \(S^k_N = \sum_{i=1}^{N} b^{(N)}_{ik}\) as the value of the \(k^{\text{th} }\) diagonal element of \(T\), i.e., \(T_{k k}\). Hence, as \(N \to \infty \), for all \(k\), \(T_{k k} \overset{D}{\to} Z_k\) where \(Z_k \overset{\text{i.i.d.} }{\sim } \operatorname{Pois}(1) \). By using the \hyperref[thm:Skorokhod-representation]{Skorokhod's representation theorem}, we can establish that \(T \overset{\text{a.s.} }{\to} \diag (Z_1, \dots , Z_n)\),\footnote{Note that \(X_n \overset{D}{\to} X\) and \(Y_n \overset{D}{\to} Y\) doesn't imply \((X_n, Y_n) \overset{D}{\to} (X, Y)\).} therefore
			      \[
				      \mathbb{E}_{}[\lambda _1 (T)]
				      = \mathbb{E}_{}\left[\max _{1 \leq k \leq n} T_{k k}\right]
				      \to \mathbb{E}_{}\left[\max _{1 \leq k \leq n} Z_k \right].
			      \]
			      Finally, the expectation of maximum of independent Poisson is thoroughly studied~\cite{kimber1983note,briggs2009notedistributionmaximumset}, which is highly concentrated exactly around \(\log n / \log \log n\).
		      \end{explanation}

		      \begin{remark}
			      We note that the above claim doesn't require \(n\) to vary with \(N\).
		      \end{remark}

		      Finally, the bound in \hyperref[ex5.4.11]{Exercise 5.4.11} will fail if the logarithmic factors were removed becomes obvious after a direct substitution. Indeed, since \(\lVert X_i \rVert = 1 \eqqcolon K\), \hyperref[ex5.4.11]{Exercise 5.4.11} states that
		      \[
			      \mathbb{E}_{}\left[\left\lVert \sum_{i=1}^{N} X_i \right\rVert \right]
			      \lesssim \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i^2] \right\rVert ^{1 / 2} ,
		      \]
		      where the logarithmic factors were removed along with \(K = 1\). Now, using the bound for \(S \coloneqq \sum_{i=1}^{N} X_i\) we have, with the observation that \(X_i^2 = X_i\) and \(\mathbb{E}_{}[X_i^2] = \mathbb{E}_{}[X_i] = \diag (1 / n, \dots , 1 / n)\), we see that the bound becomes
		      \[
			      \frac{\log n}{\log \log n}
			      \lesssim \sqrt{\frac{N}{n} },
		      \]
		      which is clear not valid.
	\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.15]\label{ex5.4.15}
	Let \(X_1, \dots , X_N\) be independent, mean zero, \(m \times n\) random matrices, such that \(\lVert X_i \rVert \leq K\) almost surely for all \(i\). Prove that for \(t \geq 0\), we have
	\[
		\mathbb{P} \left( \left\lVert \sum_{i=1}^{N} X_i \right\rVert \geq t \right)
		\leq 2 (m+n) \exp (- \frac{t^2 / 2}{\sigma ^2 + Kt / 3}),
	\]
	where
	\[
		\sigma ^2
		= \max \left( \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i ^{\top} X_i] \right\rVert , \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i X_i ^{\top} ] \right\rVert \right) .
	\]
\end{problem*}
\begin{answer}
	Consider the following \(N\) independent \((m+n) \times (m+n)\) symmetric, mean \(0\) matrices
	\[
		X_i^{\prime} \coloneqq \begin{pmatrix}
			0_{n\times n} & X_i ^{\top}    \\
			X_i           & 0_{m \times m} \\
		\end{pmatrix}.
	\]
	To apply the matrix Bernstein's inequality (Theorem 5.4.1), we need to show that \(\lVert X_i ^{\prime} \rVert \leq K^{\prime} \) for some \(K^{\prime} \), where we know that \(\lVert X_i \rVert \leq K\). However, it's easy to see that since \(\lVert X_i \rVert = \lVert X_i ^{\top} \rVert \), we have \(\lVert X_i^{\prime} \rVert \leq K\) as well since the characteristic equation for \(X_i ^{\prime} \) is
	\[
		\det (X_i^{\prime} - \lambda I)
		= \det (\begin{pmatrix}
				-\lambda I & X_i ^{\top} \\
				X_i        & -\lambda I  \\
			\end{pmatrix} )
		= \det (\lambda ^2 I - X_i ^{\top} X_i)
		= 0,
	\]
	so \(\lVert X_i^{\prime} \rVert = \sqrt{\lVert X_i ^{\top} X_i \rVert } \leq K\). Hence, from matrix Bernstein's inequality, for every \(t \geq 0\),
	\[
		\mathbb{P} \left( \left\lVert \sum_{i=1}^{N} X_i \right\rVert \geq t \right)
		\leq 2 (m+n) \exp (- \frac{t^2 / 2}{\sigma ^2 + Kt / 3}),
	\]
	where \(\sigma ^2 = \lVert \sum_{i=1}^{N} \mathbb{E}_{}[(X_i ^{\prime}) ^2] \rVert \). A quick calculation reveals that
	\[
		(X_i^{\prime} )^2
		= \begin{pmatrix}
			0   & X_i ^{\top} \\
			X_i & 0           \\
		\end{pmatrix}\begin{pmatrix}
			0   & X_i ^{\top} \\
			X_i & 0           \\
		\end{pmatrix}
		= \begin{pmatrix}
			X_i ^{\top} X_i & 0               \\
			0               & X_i X_i ^{\top} \\
		\end{pmatrix},
	\]
	hence we have
	\[
		\sigma ^2
		= \max \left( \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i ^{\top} X_i] \right\rVert , \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i X_i ^{\top} ] \right\rVert \right),
	\]
	which completes the proof.
\end{answer}
