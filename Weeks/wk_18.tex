\week{18}{29 Jun.\ 2024}{Tighter Bounds on Sub-Gaussian Matrices}

\section{Matrix Bernstein's inequality}
\begin{problem*}[Exercise 5.4.3]\label{ex5.4.3}
	\begin{enumerate}[(a)]
		\item\label{ex5.4.3:a} Consider a polynomial
		      \[
			      f(x)
			      = a_0 + a_1 x + \dots + a_p x^p.
		      \]
		      Check that for a matrix \(X\), we have
		      \[
			      f(X)
			      = a_0 I + a_1 X + \dots + a_p X^p .
		      \]
		      In the right side, we use the standard rules for matrix addition and multiplication, so in particular, \(X^p = X \cdots X\) (\(p\) times) there.
		\item\label{ex5.4.3:b} Consider a convergent power series expansion of \(f\) about \(x_0\):
		      \[
			      f(x)
			      = \sum_{k=1}^{\infty} a_k (x - x_0)^k.
		      \]
		      Check that the series of matrix terms converges, and
		      \[
			      f(X)
			      = \sum_{k=1}^{\infty} a_k (X - x_0 I)^k.
		      \]
	\end{enumerate}
\end{problem*}
\begin{answer}
	Let \(X\eqqcolon U\varLambda U^{\top}\) be the symmetric eigendecomposition of \(X\).
	\begin{enumerate}[(a)]
		\item Since \(X^k = U \varLambda U^{\top} \cdots U\varLambda U^{\top}=U\varLambda I\cdots I\varLambda U^{\top}=U\varLambda ^kU^{\top}\) for all \(k\geq 0\), then
		      \[
			      f(X)
			      = U f(\varLambda ) U^{\top}
			      =U\left( \sum_{k=0}^pa_k\varLambda ^k \right) U^{\top}
			      = \sum_{k=0}^{p} a_k U\varLambda ^k U^{\top}
			      =\sum_{k=0}^{p} a_k X^k.
		      \]
		\item Since \(X - x_0 I = U(\varLambda -x_0 I) U^{\top}\), then by \hyperref[ex5.4.3:a]{(a)},
		      \[
			      f(X)
			      = U \left( \sum_{k=1}^{\infty} a_k (\varLambda -x_0 I)^k \right) U^{\top}
			      =\sum_{k=1}^{\infty} a_k U(\varLambda -x_0 I)^k U^{\top}
			      =\sum_{k=0}^{\infty} a_k(X-x_0 I)^k.
		      \]
	\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.5]\label{ex5.4.5}
	Prove the following properties.
	\begin{enumerate}[(a)]
		\item\label{ex5.4.5:a} \(\lVert X \rVert \leq t\) if and only if \(-t I \preceq X \preceq t I\).
		\item\label{ex5.4.5:b} Let \(f, g \colon \mathbb{R} \to \mathbb{R} \) be two functions. If \(f(x) \leq g(x)\) for all \(x \in \mathbb{R} \) satisfying \(\lvert x \rvert \leq K\), then \(f(X) \preceq g(X)\) for all \(X\) satisfying \(\lVert X \rVert \leq K\).
		\item\label{ex5.4.5:c} Let \(f \colon \mathbb{R} \to \mathbb{R} \) be an increasing function and \(X, Y\) are commuting matrices. Then \(X \preceq Y\) implies \(f(X) \preceq f(Y)\).
		\item\label{ex5.4.5:d} Given an example showing that property \hyperref[ex5.4.5:c]{(c)} may fail for non-commuting matrices.
		\item\label{ex5.4.5:e} In the following parts of the exercise, we develop weaker versions of property \hyperref[ex5.4.5:c]{(c)} that hold for arbitrary, nor necessarily commuting, matrices. First, show that \(X \preceq Y\) always implies \(\tr f(X) \leq \tr f(Y)\) for any increasing function \(f \colon \mathbb{R} \to \mathbb{R} \).
		\item\label{ex5.4.5:f} Show that \(0 \preceq X \preceq Y\) implies \(X ^{-1} \preceq Y ^{-1} \) if \(X\) is invertible.
		\item\label{ex5.4.5:g} Show that \(0 \preceq X \preceq Y\) implies \(\log X \preceq \log Y\).
	\end{enumerate}
\end{problem*}
\begin{answer}
	Let \(X \eqqcolon U \varLambda U^{\top}\) and \(Y\eqqcolon V M V^{\top}\) denote the symmetric eigendecompositions of \(X\) and \(Y\), respectively. Additionally, let \(\lambda \coloneqq \diag(\varLambda )\) and \(\mu\coloneqq \diag(M)\) in \(\mathbb{R} ^n\).
	\begin{enumerate}[(a)]
		\item By the Courant-Fisher min-max theorem w.r.t.\ \(\lambda_1\) and \(\lambda_n\),
		      \[
			      \lVert X \rVert \leq t
			      \iff - t \mathbbm{1}  \leq \lambda \leq t \mathbbm{1}
			      \iff t \mathbbm{1}  \pm \lambda \geq 0
			      \iff tI \pm X \succeq 0
			      \iff -tI \preceq X \preceq tI.
		      \]
		\item Since \(\lvert \lambda \rvert \leq K \mathbbm{1} \), then \(g(\lambda)-f(\lambda)\geq 0\). This implies that \(g(X)-f(X)=U(g(\varLambda )-f(\varLambda ))U^{\top}\) has non-negative eigenvalues. Therefore, \(g(X)\succeq f(X)\).
		\item Since \(X\) and \(Y\) are symmetric and commute, then \(Y\) admits an eigendecomposition with \(V=U\). This implies \(\lambda\leq\mu\). It follows that \(f(\mu)-f(\lambda)\geq 0\), so
		      \(f(Y)-f(X)=U(f(M)-f(\varLambda ))U^{\top}\) has non-negative eigenvalues. Therefore, \(f(X) \preceq f(Y)\).
		\item We see that
		      \[
			      \lambda \left( \begin{pmatrix}
					      4 & 2 \\
					      2 & 4 \\
				      \end{pmatrix} - \begin{pmatrix}
					      3 & 0 \\
					      0 & 0 \\
				      \end{pmatrix} \right) = \{ 5, 0 \} ,
		      \]
		      while
		      \[
			      \lambda \left( \begin{pmatrix}
					      4 & 2 \\
					      2 & 4 \\
				      \end{pmatrix}^3 - \begin{pmatrix}
					      3 & 0 \\
					      0 & 0 \\
				      \end{pmatrix}^3 \right) = \left\{ \frac{\sqrt{43993} + 197}{2} , - \frac{\sqrt{43993} - 197}{2}\right\} .
		      \]
		\item Since \(X-Y \preceq 0\), then by the Courant-Fisher min-max theorem, for any \(i = 1,\dots,n\),
		      \[
			      \begin{split}
				      \lambda_i-\mu_i
				       & = \max_{\dim E=i}\min_{v\in S(E)}v^{\top} X v-\max_{\dim E=i}\min_{v\in S(E)} v^{\top} Y v      \\
				       & \leq \max_{\dim E=i} \left( \min_{v\in S(E)} v^{\top} X v - \min_{v\in S(E)}v^{\top}Y v \right) \\
				       & \leq \max_{\dim E=i}\max_{v\in S(E)} \left( v^{\top} X v - v^{\top} Y v \right)
				      = \max_{\dim E=i} \max_{v\in S(E)} v^{\top} (X-Y) v
				      \leq 0
			      \end{split}
		      \]
		      Since \(f\) is increasing, then \(f(\lambda_i)\leq f(\mu_i)\) for all \(i\). It follows that
		      \[
			      \tr f(X)
			      = \sum_{i=1}^{n} f(\lambda_i)
			      \leq \sum_{i=1}^{n} f(\mu_i)
			      =\tr f(Y).
		      \]
		\item Since \(X\preceq Y\), then \(I = X^{-1/2} X X^{-1/2} \preceq X^{-1/2} Y X^{-1/2}\). This implies \(\lambda(X^{-1/2} Y X^{-1/2}) \geq 1\). Thus, \(\lambda(X^{1/2} Y^{-1} X^{1/2}) = \lambda^{-1}(X^{-1/2} Y X^{-1/2})\leq 1\), so \(X^{1/2} Y^{-1} X^{1/2} \preceq I\). It follows that
		      \[
			      Y^{-1}
			      = X^{-1/2}(X^{1/2} Y^{-1} X^{1/2}) X^{-1/2}
			      \preceq X^{-1/2} I X^{-1/2}
			      = X^{-1}.
		      \]
		\item By \hyperref[ex5.4.5:f]{(f)}, \((X+tI)^{-1} \succeq (Y+tI)^{-1}\) for \(t \geq 0\). Since \(\log z = \at{\log \frac{1+t}{z+t}}{t = 0}{t = \infty } = \int_{0}^{\infty} \frac{1}{1+t} - \frac{1}{z+t} \,\mathrm{d} t\), then
		      \[
			      \log X
			      = \int_{0}^{\infty} ((1+t)^{-1} I - (X + tI)^{-1} ) \,\mathrm{d} t
			      \preceq \int_{0}^{\infty} ((1+t)^{-1}I-(Y+tI)^{-1}) \,\mathrm{d} t
			      = \log Y.
		      \]
	\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.6]\label{ex5.4.6}
	Let \(X\) and \(Y\) be \(n \times n\) symmetric matrices.
	\begin{enumerate}[(a)]
		\item\label{ex5.4.6:a} Show that if the matrices commute, i.e., \(XY = YX\), then
		      \[
			      e^{X + Y}
			      = e^X e^Y.
		      \]
		\item\label{ex5.4.6:b} Find and example of matrices \(X\) and \(Y\) such that
		      \[
			      e^{X + Y}
			      \neq e^X e^Y.
		      \]
	\end{enumerate}
\end{problem*}
\begin{answer}
	\begin{enumerate}[(a)]
		\item Since \(X\) and \(Y\) commute, by the binomial theorem and the substitution \(i \coloneqq k-j\),
		      \[
			      e^{X+Y}
			      = \sum_{k=0}^{\infty} \frac{(X+Y)^k}{k!}
			      = \sum_{k=0}^{\infty} \frac{1}{k!} \sum_{j=0}^{k} \frac{k!}{(k-j)!j!} X^{k-j}Y^{j}
			      = \sum_{i=0}^{\infty} \frac{X^i}{i!} \sum_{j=0}^{\infty} \frac{Y^j}{j!}
			      =e^{X}e^{Y}.
		      \]
		\item For \(X \coloneqq \begin{pmatrix}
			      1 & 0  \\
			      0 & -1 \\
		      \end{pmatrix}\) and \(Y\coloneqq \begin{pmatrix}
			      0 & 1 \\
			      1 & 0 \\
		      \end{pmatrix}\),
		      \[
			      e^{X+Y} = \begin{pmatrix}
				      \cosh \sqrt{2} + \frac{\sinh \sqrt{2}}{\sqrt{2} } & \frac{\sinh \sqrt{2}}{\sqrt{2} }                  \\
				      \frac{\sinh \sqrt{2}}{\sqrt{2} }                  & \cosh \sqrt{2} - \frac{\sinh \sqrt{2}}{\sqrt{2} }
			      \end{pmatrix},\quad
			      e^X e^Y = \frac{1}{2} \begin{pmatrix}
				      e^2 + 1    & e^2 - 1    \\
				      1 - e^{-2} & 1 + e^{-2}
			      \end{pmatrix}.
		      \]
	\end{enumerate}
\end{answer}

\begin{problem*}[Exercise 5.4.11]\label{ex5.4.11}
	Let \(X_1, \dots , X_N\) be independent, mean zero, \(n \times n\) symmetric random matrices, such that \(\lVert X_i \rVert \leq K\) almost surely for all \(i\). Deduce from Bernstein's inequality that
	\[
		\mathbb{E}_{}\left[\left\lVert \sum_{i=1}^{N} X_i \right\rVert \right]
		\lesssim \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i^2] \right\rVert ^{1 / 2} \sqrt{1 + \log n} + K(1 + \log n).
	\]
\end{problem*}
\begin{answer}
	Let \(\sigma^2 \coloneqq \lVert {\sum_{i=1}^N \mathbb{E}_{}[X_i^2] } \rVert \). By the matrix Berstein's inequality, for every \(u > 0\), with the substitution \(t \coloneqq c^{-1 / 2} \sigma \sqrt{u + \log n} + c^{-1} K (u+\log n)\),
	\[
		\mathbb{P} \left(\left\lVert {\sum_{i=1}^{N} X_i} \right\rVert \geq t \right)
		\leq 2 n e^{-c\min\left( \frac{t^2}{\sigma^2}, \frac{t}{K} \right)}
		\leq 2 n e^{-(u+\log n)}
		= 2 e^{-u}.
	\]
	Then by Lemma 1.2.1,
	\[
		\begin{split}
			 & \mathbb{E}_{}\left[\left\lVert \sum_{i=1}^{N} X_i \right\rVert \right]                                                                                                                                                                   \\
			 & = \left( \int_{0}^{c^{-1 / 2}\sigma\sqrt{1+\log n}+c^{-1}K(1+\log n)}+\int_{c^{-1 / 2}\sigma\sqrt{1+\log n}+c^{-1}K(1+\log n)}^{\infty} \right) \mathbb{P} \left( \left\lVert \sum_{i=1}^NX_i \right\rVert \geq t \right) \,\mathrm{d} t \\
			 & \leq \int_{0}^{c^{-1 / 2}\sigma \sqrt{1+\log n}+c^{-1}K(1+\log n)} 1 \,\mathrm{d} t + \int_{c^{-1 / 2}\sigma\sqrt{1+\log n}+c^{-1}K(1+\log n)}^{\infty} 2 e^{-u} \,\mathrm{d} t                                                          \\
			 & = c^{-1 / 2}\sigma\sqrt{1+\log n}+c^{-1}K(1+\log n) + \int_{1}^{\infty} 2 e^{-u} \left(\frac{2^{-1}c^{-1 / 2}\sigma}{\sqrt{u+\log n}}+c^{-1}K \right)\,\mathrm{d} u                                                                      \\
			 & \leq c^{-1 / 2}\sigma\sqrt{1+\log n} + c^{-1}K(1+\log n) + \int_{1}^{\infty} 2 e^{-u} \left(\frac{2^{-1}c^{-1 / 2}\sigma}{\sqrt{1+\log n}}+c^{-1}K\right) \,\mathrm{d} u                                                                 \\
			 & = c^{-1 / 2}\sigma\sqrt{1+\log n} + c^{-1}K(1+\log n)+2e^{-1}\left(\frac{2^{-1}c^{-1 / 2}\sigma}{\sqrt{1+\log n}}+c^{-1}K\right)                                                                                                         \\
			 & \lesssim \sigma\sqrt{1+\log n}+K(1+\log n),
		\end{split}
	\]
	which is exactly what we want to show.
\end{answer}

\begin{problem*}[Exercise 5.4.12]\label{ex5.4.12}
	Let \(\varepsilon _1, \dots , \varepsilon _n\) be independent symmetric Bernoulli random variables and let \(A_1, \dots , A_N\) be symmetric \(n \times n\) matrices (deterministic). Prove that, for any \(t \geq 0\), we have
	\[
		\mathbb{P} \left( \left\lVert \sum_{i=1}^{N} \varepsilon _i A_i \right\rVert \geq t \right)
		\leq 2 n \exp (-t^2 / 2 \sigma ^2),
	\]
	where \(\sigma ^2 = \lVert \sum_{i=1}^{N} A_i^2 \rVert \).
\end{problem*}
\begin{answer}
	Let \(\sigma^2 \coloneqq \lVert \sum_{i=1}^{N} A_i^2 \rVert \) and \(\lambda \coloneqq t / \sigma ^2 \geq 0\). By \hyperref[ex2.2.3]{Exercise 2.2.3},
	\[
		\tr e^{\sum_{i=1}^{N} \log \mathbb{E}_{}[e^{\lambda\varepsilon_iA_i}] }
		= \tr e^{\sum_{i=1}^{N} \log\cosh(\lambda A_i)}
		\leq \tr e^{\sum_{i=1}^{N} \frac{\lambda^2}{2} A_i^2}
		\leq n e^{\frac{\lambda^2}{2} \lambda_{\max} (\sum_{i=1}^{N} A_i^2)}
		= n e^{\frac{\lambda^2\sigma^2}{2}}
	\]
	Then by the Chernoff bound and Lieb's inequality,
	\[
		\begin{split}
			\mathbb{P} \left( \lambda_{\max} \left( \sum_{i=1}^{N} \varepsilon_i A_i \right) \geq t \right)
			 & \leq e^{-\lambda t} \mathbb{E}_{}[e^{\lambda \cdot \lambda_{\max}(\sum_{i=1}^{N}\varepsilon_i A_i)}] \\
			 & = e^{-\lambda t} \tr e^{\sum_{i=1}^{N} \log \mathbb{E}_{}[e^{\lambda\varepsilon_iA_i}]}
			\leq e^{-\lambda t} n e^{\frac{\lambda^2\sigma^2}{2}}
			= n e^{-\frac{t^2}{2\sigma^2}}.
		\end{split}
	\]
	Similarly, \(\mathbb{P} (\lambda_{\min}(\sum_{i=1}^{N} \varepsilon_i A_i)\leq -t) \leq n e^{-\frac{t^2}{2\sigma^2}}\).
\end{answer}

\begin{problem*}[Exercise 5.4.13]\label{ex5.4.13}
	Let \(\varepsilon _1, \dots , \varepsilon _N\) be independent symmetric Bernoulli random variables and let \(A_1, \dots , A_N\) be symmetric \(n \times n\) matrices (deterministic).
	\begin{enumerate}
		\item\label{ex5.4.13:a} Prove that
		      \[
			      \mathbb{E}_{}\left[\left\lVert \sum_{i=1}^{N} \varepsilon _i A_i \right\rVert \right]
			      \leq C \sqrt{1 + \log n} \left\lVert \sum_{i=1}^{N} A_i^2 \right\rVert ^{1 / 2}.
		      \]
		\item\label{ex5.4.13:b} More generally, prove that for every \(p \in [1, \infty )\), we have
		      \[
			      \left( \mathbb{E}_{}\left[\left\lVert \sum_{i=1}^{N} \varepsilon _i A_i \right\rVert ^p \right] \right) ^{1 / p}
			      \leq C \sqrt{p + \log n} \left\lVert \sum_{i=1}^{N} A_i^2 \right\rVert ^{1 / 2}.
		      \]
	\end{enumerate}
\end{problem*}
\begin{answer}
	Since \hyperref[ex5.4.13:a]{(a)} follows from \hyperref[ex5.4.13:b]{(b)} with \(p=1\), we will only prove \hyperref[ex5.4.13:b]{(b)} here. As the inequality trivially holds for \(n=1\) with \(C=1\), let's assume \(n\geq 2\) from now on.

	Note that if \(1 \leq p \leq 2\), then by Stirling's approximation \(\Gamma(z) \leq \sqrt{\frac{2\pi}{z}} \left( \frac{z}{e} \right)^{z} e^{\frac{1}{12z}}\),
	\[
		\left( \int_{0}^{\infty} e^{-s}(\log(2n)+s)^{\frac{p}{2} - 1}\,\mathrm{d} s \right) ^{1 / p}
		\leq \left( \int_{0}^{\infty} e^{-s}(0+s)^{\frac{p}{2} - 1}\,\mathrm{d} s \right) ^{1 / p}
		= \Gamma \left( \frac{p}{2} \right) ^{1 / p}
		\leq \frac{\pi^{\frac{1}{2p}} p^{\frac{p-1}{2p}}}{2^{\frac{1}{2} - \frac{1}{p}} e^{\frac{1}{2} - \frac{1}{6 p^2}}},
	\]
	and that if \(p>2\), then by Minkowski's inequality (and the same Stirling's approximation),
	\begin{align*}
		\left( \int_{0}^{\infty} e^{-s}(\log(2n)+s)^{\frac p2-1}\,\mathrm{d} s \right)^{1 / p}
		 & =\left( \int_{0}^{\infty} \big( e^{-\frac{s}{p}} (\log(2n)+s)^{\frac{1}{2} - \frac{1}{p}}\big)^{p} \,\mathrm{d} s \right)^{1 / p}                                                                                                    \\
		\shortintertext{since \(p > 2\), and as \(x, y > 0\), we have \((x + y)^{\frac{1}{2} - \frac{1}{p}}\leq x^{\frac{1}{2} - \frac{1}{p}} + y^{\frac{1}{2} - \frac{1}{p}}\),}
		 & \leq \left( \int_{0}^{\infty} \big( e^{-\frac{s}{p}} ((\log(2n))^{\frac{1}{2} - \frac{1}{p}} + s^{\frac{1}{2} - \frac{1}{p}}) \big)^{p} \,\mathrm{d} s \right) ^{1 / p}                                                              \\
		\shortintertext{then by Minkowski's inequality (i.e., \(\lVert f + g \rVert _{L^p} \leq \lVert f \rVert _{L^p} + \lVert g \rVert _{L^p}\)),}
		 & \leq \left( \int_{0}^{\infty} (e^{-\frac{s}{p}}(\log(2n))^{\frac{1}{2}-\frac{1}{p}})^p \,\mathrm{d} s \right)^{1 / p} + \left( \int_{0}^{\infty} (e^{-\frac{s}{p}} s^{\frac{1}{2} - \frac{1}{p}})^{p} \,\mathrm{d} s \right)^{1 / p} \\
		\shortintertext{then by some direct calculations,}
		 & = (\log(2n))^{\frac{1}{2} - \frac{1}{p}} \left( \int_{0}^{\infty} e^{-s} \,\mathrm{d} s \right)^{1 / p} + \left( \int_{0}^{\infty} e^{-s} s^{\frac{p}{2} - 1} \,\mathrm{d} s \right)^{1 / p}                                         \\
		 & = (\log(2n))^{\frac{1}{2} - \frac{1}{p}} + \Gamma\left( \frac{p}{2} \right) ^{1 / p}                                                                                                                                                 \\
		 & \leq (\log(2n))^{\frac{1}{2} - \frac{1}{p}} + \frac{\pi^{\frac{1}{2p}} p^{\frac{p-1}{2p}}}{2^{\frac{1}{2} - \frac{1}{p}} e^{\frac{1}{2} - \frac{1}{6p^2}}}.
	\end{align*}
	Let \(\sigma^2\coloneqq \lVert \sum_{i=1}^{N} A_i^2 \rVert \). By \hyperref[ex5.4.12]{Exercise 5.4.12}, for any \(t\geq 0\),
	\[
		\mathbb{P} \left( \left\lVert \sum_{i=1}^{N} \varepsilon_i A_i \right\rVert ^p \geq t \right)
		= \mathbb{P} \left( \left\lVert \sum_{i=1}^{N} \varepsilon_i A_i \right\rVert \geq t^{1 / p} \right)
		\leq 2 n e^{-\frac{t^{2/p}}{2\sigma^2}}.
	\]
	Then with the substitution \(t\eqqcolon \big(\sigma\sqrt{2(\log(2n)+s)}\big)^p\), by Lemma~1.2.1 and Minkowski's inequality,
	\[
		\begin{split}
			\left( \mathbb{E}_{}\left[\left\lVert \sum_{i=1}^N\varepsilon_iA_i \right\rVert ^p\right] \right)^{1 / p}
			 & = \left( \left( \int_{0}^{\big(\sigma\sqrt{2\log(2n)}\big)^p} + \int_{\big(\sigma\sqrt{2\log(2n)}\big)^p}^{\infty} \right) \mathbb{P} \left( \left\lVert \sum_{i=1}^{N} \varepsilon_i A_i \right\rVert ^p \geq t \right) \,\mathrm{d} t \right)^{1 / p}                       \\
			 & \leq \left( \int_{0}^{\big(\sigma\sqrt{2\log(2n)}\big)^p} 1 \,\mathrm{d} t + \int_{\big(\sigma\sqrt{2\log(2n)}\big)^p}^{\infty} 2 n e^{-\frac{t^{2/p}}{2\sigma^2}} \,\mathrm{d} t \right) ^{1 / p}                                                                            \\
			 & = \left( \big(\sigma\sqrt{2\log(2n)}\big)^p + \int_{0}^{\infty} e^{-s}\frac{(\sqrt{2}\sigma)^pp}2(\log(2n)+s)^{\frac{p}{2} - 1} \,\mathrm{d} s \right)^{1 / p}                                                                                                                \\
			 & = \sqrt{2}\sigma \left( \sqrt{\log(2n)}^{p}+\frac{p}2\!\int_{0}^{\infty}\!\!e^{-s}(\log(2n)+s)^{\frac p2-1}\,\mathrm{d} s \right) ^{1 / p}                                                                                                                                    \\
			 & \leq \sqrt{2}\sigma \left( \sqrt{\log(2n)} + \left( \frac{p}{2} \right)^{1 / p} \left( \int_{0}^{\infty} e^{-s}(\log(2n)+s)^{\frac{p}{2} - 1}\,\mathrm{d} s \right) ^{1 / p} \right)                                                                                          \\
			\shortintertext{plugging in the bound we have established in the beginning,}
			 & \leq \sqrt{2}\sigma \left( \sqrt{\log(2n)} + \left( \frac{p}{2} \right) ^{1 / p} \left( (\log(2n))^{\frac{1}{2} - \frac{1}{p}} \mathbbm{1}_{p>2} + \frac{\pi^{\frac{1}{2p}} p^{\frac{p-1}{2p}}}{2^{\frac{1}{2}-\frac{1}{p}} e^{\frac{1}{2} - \frac{1}{6p^2}}} \right) \right) \\
			 & = \sqrt{2}\sigma \left( \left( 1+ \left( \frac{p}{2} \right) ^{1 / p}(\log(2n))^{-\frac{1}{p}}\mathbbm{1}_{p>2} \right) \sqrt{\log(2n)} + \frac{\pi^{\frac{1}{2p}}p^{\frac{p+1}{2p}}}{\sqrt{2}e^{\frac{1}{2} - \frac{1}{6p^2}}} \right)                                       \\
			 & \leq\sqrt{2}\sigma \left( (1+e^{\frac{1}{e\log(16)}})\sqrt{\log(2n)} + \frac{\sqrt{\pi}}{\sqrt{2}e^{\frac{1}{3}}}\sqrt{p} \right)                                                                                                                                             \\
			 & \asymp \sigma\sqrt{p+\log n},
		\end{split}
	\]
	which is exactly what we want to show.
\end{answer}

\begin{problem*}[Exercise 5.4.14]\label{ex5.4.14}
	Let \(X\) be an \(n\times n\) random matrix that takes values \(e_ke_k^{\top}\), \(k=1,\dots,n\), with probability \(1/n\) each. (Here \((e_k)\) denotes the standard basis in \(\mathbb{R}^n\).) Let \(X_1,\dots,X_N\) be independent copies of \(X\). Consider the sum \(S=\sum_{i=1}^N X_i\), which is a diagonal matrix.
	\begin{enumerate}[(a)]
		\item\label{ex5.4.14:a} Show that the entry \(S_{ii}\) has the same distribution as the number of balls in \(i\)-th bin when \(N\) balls are thrown into \(n\) bins independently.
		\item\label{ex5.4.14:b} Relating this to the classical coupon collector's problem, show that if \(N \asymp n\), then
		      \[
			      \mathbb{E}\Vert S \Vert\asymp \frac{\log n}{\log\log n}.
		      \]
		      Deduce that the bound in \hyperref[ex5.4.11]{Exercise 5.4.11} would fail if the logarithmic factors were removed from it.
	\end{enumerate}
\end{problem*}
\begin{answer}
	\begin{enumerate}[(a)]
		\item We see that \(X = e_k e_k ^{\top} \) with \(k\) being chosen uniformly randomly among \([n]\), where \(e_k e_k ^{\top} \) is a matrix with all \(0\)'s except the \(k^{\text{th} }\) diagonal element being \(1\). Hence, by interpreting each \(X_i\) as ``throwing a ball into \(n\) bins,'' \(S_{k k}\) records the number of balls in the \(k^{\text{th} }\) bin when \(N\) balls are thrown into \(n\) bins independently.
		\item We first observe that since \(S\) is diagonal, \(\lVert S \rVert = \lambda _1(S) = \max _k S_{k k}\) as all the diagonal elements are eigenvalues of \(S\). We first answer the question of how this related to the coupon collector's problem. Firstly, let's introduce the problem formally:

		      \begin{problem}[Coupon collector's problem]
		      Say we have \(n\) different types of coupons to collect, and we buy \(N\) boxes, where each box contains a (uniformly) random type of coupon. The classical \emph{coupon collector's problem} asks for the expected number of boxes (i.e., \(N\)) we need in order to collect all coupons.
		      \end{problem}

		      \begin{intuition}
			      From \hyperref[ex5.4.14:a]{(a)}, we can view \(S_{k k}\) as the number of coupons we have collected for the \(k^{\text{th}}\) type of the coupon, where \(N\) is the number of boxes we have bought.
		      \end{intuition}

		      Hence, the coupon collector's problem asks for the expected \(N\) we need for \(\lambda _n (S) = \min _{k} S_{k k} > 0\), while \hyperref[ex5.4.14:b]{(b)} is asking for the expected number of \emph{the most frequent} coupons (i.e., \(\max_{k} S_{k k}\)) we will see when buying only \(N \asymp n\) boxes.

		      Next, let's prove the upper bound and the lower bound separately. Let \(0 < c < C\) to be some constants satisfying \(N \leq Cn\) and \(n \leq c N\).

		      \begin{claim}[Upper bound]
			      \(\mathbb{E}_{}[\lVert S \rVert ] \lesssim \log n / \log \log n\).
		      \end{claim}
		      \begin{explanation}
			      We first note that \(S_{kk} \sim \operatorname{Binomial}(N, 1 / n) \) for all \(k\), so by \hyperref[ex2.4.3]{Exercise 2.4.3},\todo{Fix} for any \(m > N / n\), we have
			      \[
				      \mathbb{P} (\lVert S \rVert \geq m)
				      = \mathbb{P} (\exists k \colon S_{k k} \geq m)
				      \leq \sum_{k=1}^{n} \mathbb{P} (S_{k k} \geq m)
				      \leq 3^{\frac{N}{n} + 1 - \frac{m \log \log n}{\log n}}.
			      \]
			      Let \(L\coloneqq \left\lfloor \frac{(C+1) \log n}{\log \log n} \right\rfloor + 1 > C + 1 > N / n\), then
			      \[
				      \begin{split}
					      \mathbb{E}_{}[\lVert S \rVert ]
					       & = \left( \sum_{m=1}^{L-1} + \sum_{m=L}^{\infty} \right) \mathbb{P} (\lVert S \rVert \geq m)               \\
					       & \leq \sum_{m=1}^{L-1} 1 + \sum_{m=L}^{\infty} 3^{\frac{N}{n} + 1 - \frac{m \log \log n}{\log n}}          \\
					       & = L - 1 + \frac{3^{\frac{N}{n} + 1 - \frac{L \log \log n}{\log n}}}{1 - 3^{- \frac{\log \log n}{\log n}}}
					      \leq \frac{(C+1) \log n}{\log \log n} + \frac{3^{C + 1 - (C+1)}}{\frac{2}{3} \cdot \frac{\log \log n}{\log n}}
					      = \frac{(C + \frac{5}{2}) \log n}{\log \log n},
				      \end{split}
			      \]
			      establishing the desired upper bound.
		      \end{explanation}

		      The hard part lies in the lower bound. We will need the following fact.

		      \begin{lemma}[Maximum of Poisson~\cite{kimber1983note,briggs2009notedistributionmaximumset}]\label{lma:maximum-Poisson}
			      Given \(Y_1, \dots , Y_n \overset{\text{i.i.d.} }{\sim } \operatorname{Pois}(1) \),
			      \[
				      \mathbb{E}_{}\left[ \max _{1 \leq k \leq n} Y_k \right]
				      \asymp \frac{\log n}{\log \log n}.
			      \]
			      Such a concentration is \emph{very} tight.
		      \end{lemma}

		      \begin{claim}[Lower bound]
			      \(\mathbb{E}_{}[\lVert S \rVert ] \gtrsim \log n / \log \log n\)
		      \end{claim}
		      \begin{explanation}
			      Let \(M_T \coloneqq \mathbb{E}_{}[\max _k \{ Y_k \} \mid \sum_{k=1}^{n} Y_k = T ] \) with \(Y_1, \dots , Y_n \overset{\text{i.i.d.} }{\sim } \operatorname{Pois}(1) \). As \((Y_1, \dots , Y_n) \mid \sum_{k=1}^{n} Y_k = T \sim \operatorname{Multinomial}(T; 1 / n, \dots , 1 / n) \), we know that \(M_T\) is non-decreasing w.r.t.\ \(T\). Moreover, as \(\sum_{k=1}^{n} Y_k \sim \operatorname{Pois}(n) \), by the law of total expectation and \hyperref[lma:maximum-Poisson]{maximum of Poisson lemma},
			      \begin{align*}
				      \frac{\log n}{\log \log n}
				      \asymp \mathbb{E}_{}\left[\max _{1 \leq k \leq n} Y_k\right]
				       & = \left( \sum_{T=0}^{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} \right) \frac{e^{-n} n^T}{T!} M_T                                                     \\
				       & \leq \sum_{T=0}^{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } \frac{e^{-n} n^T}{T!} M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} \frac{e^{-n} n^T}{T!} T \\
				       & \leq M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } \cdot 1 + \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} \frac{e^{-n} n^T}{\Gamma (T)}                                                              \\
				      \shortintertext{From Stirling's approximation, \(\Gamma (z) \geq \sqrt{2\pi } z^{z - 1 / 2} e^{-z} \) for \(z > 0\),}
				       & \leq M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} \frac{e^{-n} n^T}{\sqrt{2\pi } T^{T - 1 / 2} e^{-T}}                                               \\
				       & = M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \frac{e^{-n}}{\sqrt{2\pi } } \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} \left( \frac{n e T^{\frac{1}{2T}}}{T} \right) ^T                         \\
				      \shortintertext{since for all \(x > 0\), \(x^{1 / 2x} \leq e^{1 / 2e}\), for \(x = T\), we have}
				       & \leq M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \frac{e^{-n}}{\sqrt{2\pi } } \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} \left( \frac{n e^{1 + \frac{1}{2e}}}{T} \right) ^T                    \\
				       & \leq M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \frac{e^{-n}}{\sqrt{2\pi } } \sum_{T= \lfloor n e^{2 + \frac{1}{2e}} \rfloor + 1}^{\infty} e^{-T}                                                                \\
				       & = M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor } + \frac{e^{-n - \lfloor n e^{2 + \frac{1}{2e}} \rfloor - 1}}{\sqrt{2\pi } (1 - e^{-1})},
			      \end{align*}
			      leading to
			      \[
				      M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor}
				      \gtrsim \frac{\log n}{\log \log n}
			      \]
			      as the trailing term is decreasing exponentially fast. Finally, we have
			      \[
				      M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor}
				      \leq M_{\left\lceil \frac{\lfloor n e^{2 + \frac{1}{2e}} \rfloor}{N} \right\rceil N}
				      \leq \left\lceil \frac{\lfloor n e^{2 + \frac{1}{2e}} \rfloor}{N} \right\rceil M_N
				      \leq \left\lceil \frac{n e^{2 + \frac{1}{2e}}}{N} \right\rceil M_N
				      \leq \lceil c e^{2 + \frac{1}{2e}} \rceil M_N,
			      \]
			      where the second inequality follows from the triangle inequality of \(\max \). This leads to
			      \[
				      \mathbb{E}_{}[\lVert S \rVert ]
				      = M_N
				      \geq \frac{1}{\lceil c e^{2 + \frac{1}{2e}} \rceil } M_{\lfloor n e^{2 + \frac{1}{2e}} \rfloor }
				      \gtrsim \frac{\log n}{\log \log n}
			      \]
			      as desired.
		      \end{explanation}

		      Finally, the bound in \hyperref[ex5.4.11]{Exercise 5.4.11} will fail if the logarithmic factors were removed becomes obvious after a direct substitution. Indeed, since \(\lVert X_i \rVert = 1 \eqqcolon K\), \hyperref[ex5.4.11]{Exercise 5.4.11} states that
		      \[
			      \mathbb{E}_{}\left[\left\lVert \sum_{i=1}^{N} X_i \right\rVert \right]
			      \lesssim \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i^2] \right\rVert ^{1 / 2} ,
		      \]
		      where the logarithmic factors were removed along with \(K = 1\). Now, using the bound for \(S \coloneqq \sum_{i=1}^{N} X_i\) we have, with the observation that \(X_i^2 = X_i\) and \(\mathbb{E}_{}[X_i^2] = \mathbb{E}_{}[X_i] = \diag (1 / n, \dots , 1 / n)\), we see that the bound becomes \(\sqrt{N / n} = \Theta (1)\), while the left-hand side grows as \(\log n / \log \log n \to \infty \), which is clear not valid.
	\end{enumerate}
\end{answer}

\begin{remark}[Alternative examples]
	We give another example to demonstrate the sharpness of the matrix Bernstein's inequality. Consider the following random \(n \times n\) matrix (slightly different from \(S\))
	\[
		T \coloneqq \sum_{i=1}^{N} \sum_{k=1}^{n} b_{ik}^{(N)} e_k e_k ^{\top} ,
	\]
	where \(b_{ik}^{(N)} \overset{\text{i.i.d.} }{\sim } \operatorname{Ber}(1 / N) \). Here, we view \(X_i \coloneqq \sum_{k=1}^{n} b_{ik}^{(N)}e_k e_k ^{\top} \)

	\begin{intuition}
		In expectation, \(T\) and \(S\) should behave the same. However, this is easier to work with from independence.
	\end{intuition}

	\begin{claim}
		As \(N \to \infty \), with \(Y_k \sim \operatorname{Pois}(1) \), we have
		\[
			\mathbb{E}_{}[\lambda _1(T)]
			= \mathbb{E}_{}\left[ \max _{1 \leq k \leq n} T_{k k} \right]
			\to \mathbb{E}_{}\left[ \max _{1 \leq k \leq n} Y_k \right]
			= \Theta \left( \frac{\log n}{\log \log n} \right).
		\]
		Noticeably, the above claim doesn't require \(n\) to vary with \(N\).
	\end{claim}
	\begin{explanation}
		For every \(k \in [n]\), we apply the Poisson limit theorem since as \(N \to \infty \), \(p_{N, ik} = 1 / N \to 0\) and \(\mathbb{E}_{}[S^k_N] = \mathbb{E}_{}[\sum_{i=1}^{N} b^{(N)}_{ik}] = 1 \eqqcolon \lambda \) as \(N \to \infty \). So as \(N \to \infty \), \(S^k_N \overset{D}{\to} \operatorname{Pois}(1) \).

		With a similar interpretation as in \hyperref[ex5.4.14:a]{(a)}, we can interpret \(S^k_N = \sum_{i=1}^{N} b^{(N)}_{ik}\) as the value of the \(k^{\text{th} }\) diagonal element of \(T\), i.e., \(T_{k k}\). Hence, as \(N \to \infty \), for all \(k\), \(T_{k k} \overset{D}{\to} Y_k\) where \(Y_k \overset{\text{i.i.d.} }{\sim } \operatorname{Pois}(1) \). Since \(T_{k k}\)'s are independent, we have \(T \overset{D}{\to} \diag (Z_1, \dots , Z_n)\), therefore
		\[
			\mathbb{E}_{}[\lambda _1 (T)]
			= \mathbb{E}_{}\left[\max _{1 \leq k \leq n} T_{k k}\right]
			\to \mathbb{E}_{}\left[\max _{1 \leq k \leq n} Y_k \right]
			\asymp \Theta \left( \frac{\log n}{\log \log n} \right)
		\]
		from the \hyperref[lma:maximum-Poisson]{maximum of Poisson lemma}.
	\end{explanation}
	A simple calculation of \(\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i^2] \rVert ^{1 / 2}\) reveals that the logarithmic factors can't be removed.
\end{remark}

\begin{problem*}[Exercise 5.4.15]\label{ex5.4.15}
	Let \(X_1, \dots , X_N\) be independent, mean zero, \(m \times n\) random matrices, such that \(\lVert X_i \rVert \leq K\) almost surely for all \(i\). Prove that for \(t \geq q 0\), we have
	\[
		\mathbb{P} \left( \left\lVert \sum_{i=1}^{N} X_i \right\rVert \geq q t \right)
		\leq 2 (m+n) \exp (- \frac{t^2 / 2}{\sigma ^2 + Kt / 3}),
	\]
	where
	\[
		\sigma ^2
		= \max \left( \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i ^{\top} X_i] \right\rVert , \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i X_i ^{\top} ] \right\rVert \right) .
	\]
\end{problem*}
\begin{answer}
	Consider the following \(N\) independent \((m+n) \times (m+n)\) symmetric, mean \(0\) matrices
	\[
		X_i^{\prime} \coloneqq \begin{pmatrix}
			0_{n\times n} & X_i ^{\top}    \\
			X_i           & 0_{m \times m} \\
		\end{pmatrix}.
	\]
	To apply the matrix Bernstein's inequality (Theorem 5.4.1), we need to show that \(\lVert X_i ^{\prime} \rVert \leq K^{\prime} \) for some \(K^{\prime} \), where we know that \(\lVert X_i \rVert \leq K\). However, it's easy to see that since \(\lVert X_i \rVert = \lVert X_i ^{\top} \rVert \), we have \(\lVert X_i^{\prime} \rVert \leq K\) as well since the characteristic equation for \(X_i ^{\prime} \) is
	\[
		\det (X_i^{\prime} - \lambda I)
		= \det (\begin{pmatrix}
				-\lambda I & X_i ^{\top} \\
				X_i        & -\lambda I  \\
			\end{pmatrix} )
		= \det (\lambda ^2 I - X_i ^{\top} X_i)
		= 0,
	\]
	so \(\lVert X_i^{\prime} \rVert = \sqrt{\lVert X_i ^{\top} X_i \rVert } \leq K\).

	\begin{claim}
		Actually, we have \(\lVert X_i^{\prime} \rVert = \lVert X_i \rVert \), hence \(\lVert X_i^{\prime} \rVert \leq K\).
	\end{claim}
	\begin{explanation}
		Observe that for any matrix \(A \in \mathbb{R} ^{m \times n}\), as \(\lVert A \rVert = \sqrt{\lambda _1 (A A^{\top} )} = \sqrt{\lambda _1(A ^{\top} A)} \), we have
		\[
			\lVert A \rVert
			= \sqrt{\lambda _1 \left( \begin{pmatrix}
					A^{\top} A & 0          \\
					0          & A A^{\top} \\
				\end{pmatrix} \right) }
			= \sqrt{\lambda _1 \left( \begin{pmatrix}
					0 & A^{\top} \\
					A & 0        \\
				\end{pmatrix}^2 \right) }
			= \left\lVert \begin{pmatrix}
				0 & A^{\top} \\
				A & 0        \\
			\end{pmatrix} \right\rVert.
		\]
		Plugging in \(X_i \eqqcolon A\), we're done.
	\end{explanation}

	Hence, from matrix Bernstein's inequality, for every \(t \geq q 0\),
	\[
		\mathbb{P} \left( \left\lVert \sum_{i=1}^{N} X_i \right\rVert \geq q t \right)
		\leq 2 (m+n) \exp (- \frac{t^2 / 2}{\sigma ^2 + Kt / 3}),
	\]
	where \(\sigma ^2 = \lVert \sum_{i=1}^{N} \mathbb{E}_{}[(X_i ^{\prime}) ^2] \rVert \). A quick calculation reveals that
	\[
		(X_i^{\prime} )^2
		= \begin{pmatrix}
			0   & X_i ^{\top} \\
			X_i & 0           \\
		\end{pmatrix}\begin{pmatrix}
			0   & X_i ^{\top} \\
			X_i & 0           \\
		\end{pmatrix}
		= \begin{pmatrix}
			X_i ^{\top} X_i & 0               \\
			0               & X_i X_i ^{\top} \\
		\end{pmatrix},
	\]
	hence we have
	\[
		\sigma ^2
		= \max \left( \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i ^{\top} X_i] \right\rVert , \left\lVert \sum_{i=1}^{N} \mathbb{E}_{}[X_i X_i ^{\top} ] \right\rVert \right),
	\]
	which completes the proof.
\end{answer}

\section{Application: community detection in sparse networks}


\section{Application: covariance estimation for general distributions}