\week{2}{24 Jan.\ 2024}{Basic Inequalities and Limit Theorems}

\begin{problem*}[Exercise 1.2.6]\label{ex1.2.6}
	Deduce Chebyshev's inequality by squaring both sides of the bound \(\lvert X - \mu \rvert \geq t\) and applying Markov's inequality.
\end{problem*}
\begin{answer}
	From Markov's inequality, for any \(t > 0\),
	\[
		\Pr_{}(\vert X - \mu  \vert \geq t)
		= \Pr_{}(\vert X - \mu  \vert ^2 \geq t^2)
		\leq \frac{\mathbb{E}_{}\left[\vert X - \mu \vert ^2 \right] }{t^2}
		= \frac{\sigma ^2}{t^2}.
	\]
\end{answer}

\section{Limit theorems}

\begin{problem*}[Exercise 1.3.3]\label{ex1.3.3}
	Let \(X_1, X_2, \dots \) be a sequence of i.i.d.\ random variables with mean \(\mu \) and finite variance. Show that
	\[
		\mathbb{E}_{}\left[\left\lvert \frac{1}{N} \sum_{i=1}^{N} X_i - \mu \right\rvert \right]
		= O\left( \frac{1}{\sqrt{N} } \right)
		\text{ as } N \to \infty .
	\]
\end{problem*}
\begin{answer}
	We see that
	\[
		\mathbb{E}_{}\left[\left\vert \frac{1}{N} \sum_{i=1}^{N} X_i - \mu \right\vert \right]
		\leq \sqrt{\mathbb{E}_{}\left[\left\vert \frac{1}{N} \sum_{i=1}^{N} X_i - \mu \right\vert ^2 \right] }
		= \sqrt{\Var_{}\left[\frac{1}{N} \sum_{i=1}^{N} X_i \right] }
		= \frac{\sigma}{\sqrt{N} }.
	\]
	As \(\sigma < \infty \) is a constant, the rate is exactly \(O(1 / \sqrt{N} )\).
\end{answer}